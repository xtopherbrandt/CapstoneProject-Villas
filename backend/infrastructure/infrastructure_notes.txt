Following this guide: https://aws.amazon.com/blogs/database/deploy-amazon-rds-databases-for-applications-in-kubernetes/ 
    which uses this guide: https://aws-controllers-k8s.github.io/community/docs/tutorials/rds-example/#install-the-ack-service-controller-for-rds
        which uses this guide: https://aws-controllers-k8s.github.io/community/docs/user-docs/irsa/

Cluster and Database deployment

    IAM Role For ACK-RDS-controller
        Created a custom role: ack-rds-controller following the scripts in Step 2 of https://aws-controllers-k8s.github.io/community/docs/user-docs/irsa/
        Did this manually with the account and oidc issuer hard coded. --> Not sure of the OIDC issuer is static between cluster tear-down and recreate

    Permissions Required For Deployment
        The user running the deployment scripts needs many permissions that are not well documented. The following policies are in place:
        
        AWS Managed:
            AmazonEC2ContainerRegistryFullAccess
            AmazonEC2FullAccess
            AWSCloudFormationFullAccess
            IAMFullAccess
            IAMUserChangePassword
        
        Customer Managed
            EKS_All_Access
            get-ecr-token
            IAM_Limited_Access
            RDS_Limited_Describe
            rds-delete
            rds-service-role-creation
            View-Kubernetes-Nodes

    Logical Service Layout
        The deployment scripts create a cluster.
        The AWS Controller for Kubernetes is then added to that cluster from a helm chart
            The ACK allows control of the database instance through Kubernetes
        A database instance is deployed through the ACK. It is deployed within the AWS RDS service and is visible there.
            A single database (villas) is deployed in this instance but in the future we could deploy villas-test here too.
        The villas service (python container) is then deployed to the cluster
            ** need to determine how to get the DB connection string parameters available to the container
        A kubernetes load balancer is deployed in front of the cluster.
            The ingress of the loadbalancer is the public IP of the service

AWS LoadBalancer Public URL
    Can get the URL for the load balancer, the one to use when accessing the service, from this command:
    (kubectl describe services -n villas-deploy villas-external-service) -match 'LoadBalancer Ingress'

IAM

Have two policies for Code Build:
- CodeBuildBasePolicy-Villas-backend-python-us-west-2
- CodeBuildBasePolicy-Villas-direct-build-us-west-2

These are created and sort of managed by the build project. If changes are made to the build project environment,
like adding or changing Environment Variables, then need to delete these policies and allow the build project to
recreate them (it's stupid).

The main issue is that in order for the build to run docker build it needs extra permissions. This extra permission has
been put into the ECR_Docker_Build policy which has been added to the codebuild-Villas-backend-python-service-role.

Build Machine
Need to use a linux AMD64 build machine. The aarch64 (arm64) machines don't seem to work well with the python image.
And I havn't been able to download an arm64v8/python image.

Python Base image
Have pushed a version of the Python base image to an ECR repo. This is avoid AWS from getting a rate limit error when
downloading the image from DockerHub. So we're storing it locally and pulling from the local ECR store.

There is a handy View push commands button in the ECR / Repositories / <repo> page with instructions on how to push images
to the repo. Basic process is pull from DockerHub, retag and push to ECR.

    Login: aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 899955329535.dkr.ecr.us-west-2.amazonaws.com
    Push: docker push 899955329535.dkr.ecr.us-west-2.amazonaws.com/python:latest

There is something called a pull through cache which may solve this issue.

Build Spec
    The file backend-buildspec.yml uses some environment variables that are defined in the settings for the Villas-backend-python build project. Click Edit > Environment to view.

Image Versioning
    The file named 'version' in the root of the project contains the current image version. The contents of that file are picked up by the backend-buildspec and used as the image tag
    To rollout the latest version:

    kubectl set image deployment/eks-villas-linux-deployment villas=899955329535.dkr.ecr.us-west-2.amazonaws.com/villas-ecr:<version> -n villas-deploy

Troubleshooting
    Pods
        Can run a bash shell on a pod to troubleshoot the pod.
            Get the pod name:
                (kubectl get pods -n villas-deploy) | Select-Object -First 1 -Skip 1 | Select-String -Pattern '(?<name>\S*)'
            Execute the bash shell:
                kubectl exec -it -n villas-deploy {pod_name} -- /bin/bash

            From there can run the Python interpreter
        
        Can get console logs from pod with:
            kubectl logs -n villas-deploy --all-containers=true {pod_name}